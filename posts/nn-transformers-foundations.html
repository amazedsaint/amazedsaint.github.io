<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Neural Networks & Transformers — Visual Foundations</title>
  <meta name="description" content="First‑principles, interactive visuals for the core ideas behind neural networks and transformers: linear units, nonlinearity, loss & gradient descent, softmax & cross‑entropy, dot‑product attention, positional encodings, layer norm & residuals, and receptive fields." />
  <link rel="stylesheet" href="../style.css" />
  <script>
    window.MathJax = {
      tex: { inlineMath: [["$","$"],["\\(","\\)"]], displayMath: [["$$","$$"],["\\[","\\]"]], packages: {'[+]':['base','ams']}, processEscapes: true },
      options: { skipHtmlTags: ['script','noscript','style','textarea','pre'] }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script defer src="../js/notes.js"></script>
  <script defer src="../js/connections.js"></script>
  <style>
    .section { margin: 28px 0 36px 0; padding-bottom: 16px; border-bottom: 1px solid var(--border); }
    .demo { background:#fff; border:1px solid var(--border); border-radius:10px; padding:14px; max-width:880px; margin:12px auto; }
    canvas { display:block; margin:10px auto; border:1px solid var(--border); border-radius:8px; background:#fff; }
    .controls { display:flex; flex-wrap:wrap; gap:8px; justify-content:center; margin-top:8px; }
    .controls input[type=range], .controls select { accent-color: var(--accent); }
    .controls input[type=text] { padding:6px 8px; border:1px solid var(--border); border-radius:6px; font-size:14px; }
    .note { max-width:880px; margin:8px auto 0 auto; color: var(--muted); font-size:0.95em; }
    .bar { height:10px; background:#e8ecef; border-radius:6px; overflow:hidden; position:relative; }
    .bar > span { position:absolute; left:0; top:0; bottom:0; background:#1a8917; }
  </style>
  <script>
    (function(){
      // ---------- Linear unit & decision line (2D) ----------
      function linearUnit(){
        const cv=document.getElementById('linUnit'); if(!cv) return; const ctx=cv.getContext('2d'); const W=cv.width=520, H=cv.height=320;
        // Synthetic data
        const pts=[]; for(let i=0;i<50;i++){ const x=(Math.random()*2-1), y=(Math.random()*2-1); const label = x*0.7 - y*0.4 + 0.1 + (Math.random()*0.4-0.2) > 0 ? 1:0; pts.push({x,y,label}); }
        let w0=1.0, w1=-1.0, b=0.0;
        function toPix(x,y){ return {x: W*0.5 + x*120, y: H*0.5 - y*120}; }
        function draw(){ ctx.clearRect(0,0,W,H); // axes
          ctx.strokeStyle='#eee'; ctx.beginPath(); ctx.moveTo(20,H*0.5); ctx.lineTo(W-20,H*0.5); ctx.stroke(); ctx.beginPath(); ctx.moveTo(W*0.5,20); ctx.lineTo(W*0.5,H-20); ctx.stroke();
          // points
          for(const p of pts){ const P=toPix(p.x,p.y); ctx.fillStyle=p.label? '#1a8917':'#b91c1c'; ctx.beginPath(); ctx.arc(P.x,P.y,3,0,Math.PI*2); ctx.fill(); }
          // decision line w0 x + w1 y + b = 0 → y = -(w0/w1) x - b/w1
          if(Math.abs(w1)>1e-6){ const x1=-1.2, x2=1.2; const y1=-(w0/w1)*x1 - b/w1, y2=-(w0/w1)*x2 - b/w1; const P1=toPix(x1,y1), P2=toPix(x2,y2); ctx.strokeStyle='#111'; ctx.beginPath(); ctx.moveTo(P1.x,P1.y); ctx.lineTo(P2.x,P2.y); ctx.stroke(); }
          document.getElementById('linEq').textContent = `y = ${(-w0/w1).toFixed(2)} x ${(-b/w1>=0?'+':'-')} ${Math.abs(b/w1).toFixed(2)}`;
        }
        function setW0(v){ w0=parseFloat(v); draw(); }
        function setW1(v){ w1=parseFloat(v); draw(); }
        function setB(v){ b=parseFloat(v); draw(); }
        document.getElementById('w0').oninput=(e)=> setW0(e.target.value);
        document.getElementById('w1').oninput=(e)=> setW1(e.target.value);
        document.getElementById('b').oninput=(e)=> setB(e.target.value);
        draw(); return { setW0, setW1, setB };
      }

      // ---------- Softmax & cross-entropy ----------
      function softmaxDemo(){
        const logits=[0.5, -0.2, 1.0]; let target=2; const bars=document.querySelectorAll('[data-soft]'); const lossEl=document.getElementById('xent');
        function softmax(z){ const m=Math.max(...z); const e=z.map(v=>Math.exp(v-m)); const s=e.reduce((a,b)=>a+b,0); return e.map(v=>v/s); }
        function render(){ const p=softmax(logits); bars.forEach((el,i)=>{ const v=Math.max(0,Math.min(1,p[i])); el.style.width=(v*100)+'%'; el.parentElement.querySelector('span.value').textContent=v.toFixed(3); }); const loss=-Math.log(Math.max(1e-9, p[target])); lossEl.textContent = loss.toFixed(3); }
        ['l0','l1','l2'].forEach((id,i)=>{ document.getElementById(id).oninput=(e)=>{ logits[i]=parseFloat(e.target.value); render(); }; });
        document.getElementById('tgt').onchange=(e)=>{ target=parseInt(e.target.value); render(); };
        render(); return { render };
      }

      // ---------- Dot-product attention (single head) ----------
      function attentionDemo(){
        const cv=document.getElementById('attn'); if(!cv) return; const ctx=cv.getContext('2d'); const W=cv.width=520, H=cv.height=220;
        let n=5, d=3, temp=1.0, qi=2; let Q=[],K=[];
        function rand(){ return Math.random()*2-1; }
        function init(){ Q=Array.from({length:n},()=> Array.from({length:d}, rand)); K=Array.from({length:n},()=> Array.from({length:d}, rand)); }
        function dot(a,b){ let s=0; for(let i=0;i<a.length;i++) s+=a[i]*b[i]; return s; }
        function softmax(a){ const m=Math.max(...a); const e=a.map(v=>Math.exp(v-m)); const s=e.reduce((x,y)=>x+ y,0); return e.map(v=>v/s); }
        function render(){ ctx.clearRect(0,0,W,H); const cell=26, offx=30, offy=30; // heatmap for weights of qi
          const scores=[]; for(let j=0;j<n;j++) scores.push(dot(Q[qi],K[j]) / Math.max(1e-6,temp)); const w=softmax(scores);
          for(let j=0;j<n;j++){ const x=offx + j*cell; const y=offy; const val=w[j]; ctx.fillStyle=`rgba(26,137,23,${0.15+0.75*val})`; ctx.fillRect(x,y,cell,cell); ctx.strokeStyle='#ccc'; ctx.strokeRect(x,y,cell,cell); ctx.fillStyle='#111'; ctx.font='11px ui-monospace, SFMono-Regular, Menlo, Monaco, monospace'; ctx.fillText(j.toString(), x+8, y-6); }
          ctx.fillStyle='#666'; ctx.fillText('weights for query q['+qi+']', offx, offy+cell+16);
        }
        document.getElementById('temp').oninput=(e)=>{ temp=parseFloat(e.target.value); render(); };
        document.getElementById('qpick').oninput=(e)=>{ qi=parseInt(e.target.value); render(); };
        init(); render(); return { render };
      }

      // ---------- Positional encodings (sinusoidal) ----------
      function posencDemo(){ const cv=document.getElementById('posenc'); if(!cv) return; const ctx=cv.getContext('2d'); const W=cv.width=520, H=cv.height=160; let L=32, d=8;
        function enc(pos, i){ const div = Math.pow(10000, (2*Math.floor(i/2))/d); return (i%2===0)? Math.sin(pos/div): Math.cos(pos/div); }
        function render(){ ctx.clearRect(0,0,W,H); const offx=30, offy=H*0.5; ctx.strokeStyle='#eee'; ctx.beginPath(); ctx.moveTo(20,offy); ctx.lineTo(W-20,offy); ctx.stroke(); const scaleX=(W-60)/L, scaleY=28; for(let c=0;c<Math.min(6,d);c++){ ctx.strokeStyle=`hsl(${(c*60)%360},60%,40%)`; ctx.beginPath(); for(let x=0;x<=L;x++){ const y=enc(x,c); const X=offx + x*scaleX; const Y=offy - y*scaleY - c*2; if(x===0) ctx.moveTo(X,Y); else ctx.lineTo(X,Y);} ctx.stroke(); }
        }
        document.getElementById('L').oninput=(e)=>{ L=parseInt(e.target.value); render(); };
        document.getElementById('D').oninput=(e)=>{ d=parseInt(e.target.value); render(); };
        render(); return { render };
      }

      // ---------- Layer norm (vector) ----------
      function layerNormDemo(){ const cv=document.getElementById('ln'); if(!cv) return; const ctx=cv.getContext('2d'); const W=cv.width=520, H=cv.height=150; let vec=[1.2,-0.6, 0.4, 2.0, -1.0];
        function norm(v){ const m=v.reduce((a,b)=>a+b,0)/v.length; const s=Math.sqrt(v.reduce((a,b)=>a+(b-m)*(b-m),0)/v.length+1e-6); return v.map(x=>(x-m)/s); }
        function render(){ ctx.clearRect(0,0,W,H); const offx=30, base=H*0.75, scale=20, bw=20; // original
          ctx.fillStyle='#666'; ctx.fillText('values', offx, 18); for(let i=0;i<vec.length;i++){ const h=vec[i]*scale; const x=offx+i*(bw+8); ctx.fillStyle='#3b82f6'; ctx.fillRect(x, base - Math.max(0,h), bw, Math.abs(h)); }
          const nv=norm(vec); ctx.fillStyle='#666'; ctx.fillText('layer‑norm', offx+200, 18); for(let i=0;i<nv.length;i++){ const h=nv[i]*scale; const x=offx+200+i*(bw+8); ctx.fillStyle='#1a8917'; ctx.fillRect(x, base - Math.max(0,h), bw, Math.abs(h)); }
        }
        render(); return { render };
      }

      // ---------- Receptive field: conv vs attention ----------
      function receptiveDemo(){ const cv=document.getElementById('rf'); if(!cv) return; const ctx=cv.getContext('2d'); const W=cv.width=520, H=cv.height=140; let N=12, k=3, center=6, mode='attn';
        function render(){ ctx.clearRect(0,0,W,H); const offx=20, y=H*0.5, cell=(W-40)/N; for(let i=0;i<N;i++){ const x=offx+i*cell; let highlight=false; if(mode==='attn') highlight=true; else { const half=Math.floor(k/2); highlight = (i>=center-half && i<=center+half); }
            ctx.fillStyle = highlight? (i===center? '#1a8917':'rgba(26,137,23,0.3)') : '#eee'; ctx.fillRect(x,y-14,cell-4,28); ctx.strokeStyle='#ccc'; ctx.strokeRect(x,y-14,cell-4,28); if(i===center){ ctx.fillStyle='#111'; ctx.fillText('•', x+cell*0.35, y+4); } }
          ctx.fillStyle='#666'; ctx.fillText(mode==='attn'? 'Self‑attention sees all positions' : `Conv kernel size ${k} sees local window`, offx, y+36);
        }
        document.getElementById('mode').onchange=(e)=>{ mode=e.target.value; render(); };
        document.getElementById('kern').oninput=(e)=>{ k=parseInt(e.target.value); render(); };
        document.getElementById('center').oninput=(e)=>{ center=parseInt(e.target.value); render(); };
        render(); return { render };
      }

      function start(){ linearUnit(); softmaxDemo(); attentionDemo(); posencDemo(); layerNormDemo(); receptiveDemo(); }
      if (document.readyState==='complete' || document.readyState==='interactive') start(); else document.addEventListener('DOMContentLoaded', start);
    })();
  </script>
</head>
<body>
  <div class="container">
    <header class="header">
      <h1 class="site-title"><a href="../index.html">antifold</a></h1>
      <nav class="nav">
        <a href="../index.html">Home</a>
        <a href="https://github.com/amazedsaint">GitHub</a>
        <a href="https://twitter.com/amazedsaint">Twitter</a>
      </nav>
    </header>

    <article class="article-content">
      <div class="article-header">
        <span class="tile-category">AI</span>
        <h1 class="article-title">Neural Networks & Transformers — Visual Foundations</h1>
        <div class="article-meta">September 24, 2025 • AI • Interactive</div>
        <p class="article-description">The core building blocks: linear units, nonlinearity, loss & gradient descent, softmax & cross‑entropy, dot‑product attention, positional encodings, layer norm & residuals, and receptive fields. Each section has an explainer, a formula, and a small visual.</p>
        <p class="note">Hover <span class="idea" data-note="Toolkit: dot product, linear maps, rotations. Primer: small oscillations intuition carries over to optimization landscapes.">here</span> for how this connects to earlier pieces.</p>
      </div>

      <div class="connections">
        <div class="header"><span class="title">Connections</span><span aria-hidden="true">▸</span></div>
        <div class="content">
          <ul>
            <li>Dot product & linear maps → <a href="physics-toolkit-intuition.html#vectors-and-dot-product">Toolkit</a>.</li>
            <li>Optimization landscape intuition → <a href="physics-math-primer.html#potentials-and-small-oscillations">Primer: Potentials</a>.</li>
            <li>Attention weights use softmax and dot products → see below; also <a href="physics-toolkit-intuition.html#fourier-builder">Toolkit: Fourier</a> for oscillatory ideas.</li>
          </ul>
        </div>
      </div>

      <div class="section">
        <h2 id="linear-units">Linear Units & Nonlinearity</h2>
        <p class="note">A single neuron computes a weighted sum and passes it through a nonlinearity: $$y = \phi(\mathbf w\cdot\mathbf x + b).$$ Without $\phi$, stacks of linear layers collapse to one linear map. With $\phi$ (ReLU, GELU, tanh), you can fit bends in the data.</p>
        <div class="demo">
          <canvas id="linUnit" width="520" height="320"></canvas>
          <div class="controls">
            <label>w0 <input id="w0" type="range" min="-2" max="2" step="0.01" value="1.0"/></label>
            <label>w1 <input id="w1" type="range" min="-2" max="2" step="0.01" value="-1.0"/></label>
            <label>b <input id="b" type="range" min="-1" max="1" step="0.01" value="0.0"/></label>
          </div>
          <p class="note">Decision line: <span id="linEq">—</span>. Terms: $\mathbf w=(w_0,w_1)$ are weights; $b$ is bias; $\phi$ shapes the output. The data here is linearly separable up to noise—nonlinearity lets you stack layers for curved boundaries.</p>
        </div>
      </div>

      <div class="section">
        <h2 id="softmax-crossentropy">Softmax & Cross‑Entropy</h2>
        <p class="note">Softmax turns logits into probabilities: $$p_i=\frac{e^{z_i}}{\sum_j e^{z_j}}.$$ For a target class $t$, cross‑entropy is $L=-\log p_t$ (negative log‑likelihood). Lower is better; pushing the correct logit up pulls $p_t$ closer to 1.</p>
        <div class="demo">
          <div class="controls">
            <label>logit[0] <input id="l0" type="range" min="-3" max="3" step="0.01" value="0.5"/></label>
            <label>logit[1] <input id="l1" type="range" min="-3" max="3" step="0.01" value="-0.2"/></label>
            <label>logit[2] <input id="l2" type="range" min="-3" max="3" step="0.01" value="1.0"/></label>
            <label>target <select id="tgt"><option value="0">0</option><option value="1">1</option><option value="2" selected>2</option></select></label>
          </div>
          <div class="note">probabilities</div>
          <div class="bar"><span data-soft></span></div>
          <div class="bar"><span data-soft></span></div>
          <div class="bar"><span data-soft></span></div>
          <div class="note">values: <span class="value">—</span>, <span class="value">—</span>, <span class="value">—</span></div>
          <p class="note">Cross‑entropy: <strong id="xent">—</strong>. “Terms”: $z$ are logits; $p$ are normalized probabilities; the target index controls which $p_t$ we log.</p>
        </div>
      </div>

      <div class="section">
        <h2 id="attention">Dot‑Product Attention</h2>
        <p class="note">Given queries $Q$ and keys $K$, attention scores are dot products scaled by temperature $T$: $$\text{weights}(q_i, K)=\operatorname{softmax}\big(\tfrac{q_i K^\top}{T}\big).$$ Each query picks a distribution over positions; values (not shown here) are then averaged with these weights.</p>
        <div class="demo">
          <canvas id="attn" width="520" height="220"></canvas>
          <div class="controls">
            <label>query index <input id="qpick" type="range" min="0" max="4" step="1" value="2"/></label>
            <label>temperature <input id="temp" type="range" min="0.2" max="3.0" step="0.01" value="1.0"/></label>
          </div>
          <p class="note">Lower temperature sharpens focus; higher temperature spreads it. Terms: $Q,K\in\mathbb R^{n\times d}$; dot products measure similarity; softmax normalizes.</p>
        </div>
      </div>

      <div class="section">
        <h2 id="positional-encodings">Positional Encodings</h2>
        <p class="note">Transformers add position through sinusoidal features: for position $p$ and channel $i$, $$\text{PE}(p,i)=\begin{cases}\sin\big(p/10000^{2i/d}\big) & i\text{ even}\\ \cos\big(p/10000^{2i/d}\big) & i\text{ odd}\end{cases}.$$ The pattern lets the model infer relative offsets by simple dot products.</p>
        <div class="demo">
          <canvas id="posenc" width="520" height="160"></canvas>
          <div class="controls">
            <label>length <input id="L" type="range" min="8" max="128" step="1" value="32"/></label>
            <label>channels <input id="D" type="range" min="4" max="64" step="1" value="8"/></label>
          </div>
          <p class="note">Notice how low‑frequency channels vary smoothly while higher ones oscillate quickly—together they describe position at multiple scales.</p>
        </div>
      </div>

      <div class="section">
        <h2 id="layernorm-residuals">Layer Norm & Residuals</h2>
        <p class="note">LayerNorm centers and scales each token’s features: $$\text{LN}(x)=\frac{x-\mu}{\sigma+\epsilon}\cdot\gamma+\beta,$$ stabilizing magnitude across layers. Residuals add the input back: $y = x + f(\text{LN}(x))$, keeping information flowing.</p>
        <div class="demo">
          <canvas id="ln" width="520" height="150"></canvas>
          <p class="note">Left bars: raw features; right: normalized. Terms: $\mu,\sigma$ are per‑token mean/std; $\gamma,\beta$ are learned scale/shift.</p>
        </div>
      </div>

      <div class="section">
        <h2 id="receptive-fields">Receptive Fields</h2>
        <p class="note">Convolutions see local windows; self‑attention can see the whole sequence (unless masked). The picture below toggles between both views for one position.</p>
        <div class="demo">
          <canvas id="rf" width="520" height="140"></canvas>
          <div class="controls">
            <label>mode <select id="mode"><option value="attn" selected>attention</option><option value="conv">convolution</option></select></label>
            <label>kernel <input id="kern" type="range" min="1" max="9" step="2" value="3"/></label>
            <label>center <input id="center" type="range" min="0" max="11" step="1" value="6"/></label>
          </div>
          <p class="note">Notice how attention can connect distant tokens in one step; convolution stacks multiple layers to widen its view.</p>
        </div>
      </div>

      <div class="connections">
        <div class="header"><span class="title">Connections</span><span aria-hidden="true">▸</span></div>
        <div class="content">
          <ul>
            <li>Dot products & linear maps → <a href="physics-toolkit-intuition.html#vectors-and-dot-product">Toolkit</a>.</li>
            <li>Optimization intuition → <a href="physics-math-primer.html#potentials-and-small-oscillations">Primer</a>.</li>
            <li>Softmax appears in attention & classification; oscillations echo Toolkit’s Fourier view.</li>
          </ul>
        </div>
      </div>

      <div class="related-links">
        <strong>Related:</strong>
        <a href="physics-toolkit-intuition.html">Toolkit</a> •
        <a href="physics-math-primer.html">Primer</a> •
        <a href="../index.html">Home</a>
      </div>
    </article>

    <footer class="footer">
      <p>&copy; 2025 antifold • Essays and simulations</p>
      <p class="meta-mini">By Anoop • <a href="https://twitter.com/amazedsaint">Twitter</a> • <a href="https://github.com/amazedsaint">GitHub</a> • <a href="../index.html">Home</a></p>
    </footer>
  </div>
</body>
</html>

